{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Encoders \n",
    "This notebook aims to provide a comprehensive view of all the categorical encoders available in scikit learn. A brief explanation of the working of the encoders along with short and readable examples. The example chosen is intentionally  a small dataset since a smaller dataset brings out the difference between the encoders quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First,\n",
    "#### Initialising Dummy Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small Dummy datasets provide an easy understanding of the output of each encoder. The below dataset has a dummy probability associated with few countries. The dataset consists of a list of countries with a random probability associated with them along with their binary targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import category_encoders as ce\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set\n",
    "tr=pd.DataFrame([['BR',0.9,1],\n",
    "                 ['FR',0.6,0],\n",
    "                 ['BR',0.7,1],\n",
    "                 ['JP',0.2,0],\n",
    "                 ['MA',0.4,0]], columns=['Country','prob','Target'])\n",
    "#Test set\n",
    "ts=pd.DataFrame([['BR',0.5,1],\n",
    "                 ['FR',0.2,0],\n",
    "                 ['BR',0.4,1],\n",
    "                 ['ZA',0.3,0]], columns=['Country','prob','Target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(xtr,ytr,xts,yts):\n",
    "    decisiontree = DecisionTreeClassifier()\n",
    "    decisiontree.fit(xtr,ytr)\n",
    "    y_pred=decisiontree.predict(xts)\n",
    "    print(\"Target\")\n",
    "    print(yts.values)\n",
    "    print(\"Predicted:\", y_pred)\n",
    "    print('Confusion Matrix \\n  0 1')\n",
    "    print(confusion_matrix(y_pred,yts))\n",
    "    print(\"Accuracy Score\")\n",
    "    print(accuracy_score(y_pred,yts))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Terminologies\n",
    "\n",
    " **Category** : A categorical column. Example, Destination.\n",
    " \n",
    " **Level** : Is a value in the categorical column. Example, US is a level in the Destination column.\n",
    " \n",
    " **Dummy encoding** - substitues values or numbers according to the number of levels\n",
    " \n",
    " **Contrast encoding** - compares levels against a reference level\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Encoder groups\n",
    "\n",
    "### Classic Encoders \n",
    "Simple and easily understandable form of conversions\n",
    "\n",
    "[Label Encoder](#Label-Encoder) - Ordinal \n",
    "\n",
    "[Count Encoder](#Count-Encoder) - Nominal, Ordinal\n",
    "\n",
    "[Binary Encoder](#Binary-Encoder)  - Nominal, Ordinal\n",
    "\n",
    "[Base N Encoder](#Base-N-Encoder) - Nominal, Ordinal\n",
    "\n",
    "### Contrast Encoders\n",
    "Encode against a reference level\n",
    "\n",
    "[One Hot Encoder](#One-Hot-Encoder) - Nominal , Ordinal\n",
    "\n",
    "[Sum Encoder](#Sum-Encoder) Nominal , Ordinal\n",
    "\n",
    "[Helmert Encoder](#Helmert-Encoder) -Nominal , Ordinal\n",
    "\n",
    "[Backward Difference Encoder](#Backward-Difference-Encoder) - Nominal , Ordinal\n",
    "\n",
    "[Polynomial Encoder](#Polynomial-Encoder)  - Nominal , Ordinal\n",
    "\n",
    "### Bayesian Encoders \n",
    "Encode target information\n",
    "\n",
    "[Target Encoder](#Target-Encoder) -Nominal , Ordinal\n",
    "\n",
    "[Leave One Out Encoder](#Leave-One-Out-Encoder) - Nominal , Ordinal\n",
    "\n",
    "[CatBoost Encoder](#CatBoost-Encoder) - Nominal , Ordinal\n",
    "\n",
    "[James Stein Encoder](#James-Stein-Encoder) - Nominal , Ordinal\n",
    "\n",
    "[M Estimate](#M-Estimate) - Nominal , Ordinal\n",
    "\n",
    "[Weight Of Evidence](#Weight-of-Evidence) - Nominal , Ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder\n",
    "Simply converts the levels to a list of numbers starting from 0 to n-1\n",
    "\n",
    "Advantages : Quick to implement , easy to understand\n",
    "\n",
    "Disadvantages : Infuses an unintended order in the dataset\n",
    "\n",
    "### Count Encoder\n",
    "Simply counts the number of occurences of the level\n",
    "\n",
    "Advantages : Quick to implement , easy to understand\n",
    "\n",
    "Disadvantages : Might skew the distribution if the level distribution is extreme. Special care is needed when new values are encountered in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "   Country  prob\n",
      "0        2   0.9\n",
      "1        1   0.6\n",
      "2        2   0.7\n",
      "3        1   0.2\n",
      "4        1   0.4\n",
      "Transformed Test data\n",
      "   Country  prob\n",
      "0      2.0   0.5\n",
      "1      1.0   0.2\n",
      "2      2.0   0.4\n",
      "3      NaN   0.3\n"
     ]
    }
   ],
   "source": [
    "counten= ce.CountEncoder(cols=['Country'])\n",
    "counten.fit(tr.iloc[:,:-1],tr.iloc[:,-1:].values.ravel())\n",
    "tr1=counten.transform(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr.iloc[:,-1:]\n",
    "print(\"Transformed Training data\")\n",
    "print(tr1)\n",
    "\n",
    "ts1=counten.transform(ts.iloc[:,:-1],ts.iloc[:,-1:])\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts.iloc[:,-1:]\n",
    "print(\"Transformed Test data\")\n",
    "print(ts1)\n",
    "#train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder\n",
    "\n",
    "Converts each level to a binary number. Produces one feature per category.\n",
    "\n",
    "Not suitable for high cardinal categorical features. Produces too many columns if the number of levels are high.\n",
    "Tree based models are hit hard if there are lot of levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "   Country_1  Country_2  Country_3  Country_4  prob  Target\n",
      "0          1          0          0          0   0.9       1\n",
      "1          0          1          0          0   0.6       0\n",
      "2          1          0          0          0   0.7       1\n",
      "3          0          0          1          0   0.2       0\n",
      "4          0          0          0          1   0.4       0\n",
      "Transformed Test data\n",
      "   Country_1  Country_2  Country_3  Country_4  prob  Target\n",
      "0          1          0          0          0   0.5       1\n",
      "1          0          1          0          0   0.2       0\n",
      "2          1          0          0          0   0.4       1\n",
      "3          0          0          0          0   0.3       0\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [0 0 0 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 2]\n",
      " [0 0]]\n",
      "Accuracy Score\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "OHE = ce.OneHotEncoder(cols=['Country'])\n",
    "tr1=OHE.fit_transform(tr)\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr1.iloc[:,-1:]\n",
    "\n",
    "print(\"Transformed Training data\")\n",
    "print(tr1)\n",
    "ts1=OHE.transform(ts)\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts1.iloc[:,-1:]\n",
    "\n",
    "print(\"Transformed Test data\")\n",
    "print(ts1)\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Encoder \n",
    "\n",
    "Similar to One Hot Encoding except that it is a contrast type encoder where one value is held as a reference and is encoded as -1\n",
    "\n",
    "Advantages : Lesser columns than OHE due to the reference value\n",
    "\n",
    "Disadvantages : Still produces many columns. Does not encode unseen values.. Notice the 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "   intercept  Country_0  Country_1  Country_2  prob  Target\n",
      "0          1        1.0        0.0        0.0   0.9       1\n",
      "1          1        0.0        1.0        0.0   0.6       0\n",
      "2          1        1.0        0.0        0.0   0.7       1\n",
      "3          1        0.0        0.0        1.0   0.2       0\n",
      "4          1       -1.0       -1.0       -1.0   0.4       0\n",
      "Transformed Test data\n",
      "   intercept  Country_0  Country_1  Country_2  prob  Target\n",
      "0        1.0        1.0        0.0        0.0   0.5       1\n",
      "1        1.0        0.0        1.0        0.0   0.2       0\n",
      "2        1.0        1.0        0.0        0.0   0.4       1\n",
      "3        1.0        0.0        0.0        0.0   0.3       0\n",
      "4        NaN        NaN        NaN        NaN   NaN       0\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [1 0 1 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 0]\n",
      " [0 2]]\n",
      "Accuracy Score\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SumEn= ce.SumEncoder(cols=['Country'])\n",
    "SumEn.fit(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "tr1=SumEn.transform(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr.iloc[:,-1:]\n",
    "print(\"Transformed Training data\")\n",
    "print(pd.concat([tr1,tr.iloc[:,-1:]],axis=1))\n",
    "\n",
    "\n",
    "ts1=SumEn.transform(ts.iloc[:,:-1],ts.iloc[:,-1:])\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts.iloc[:,-1:]\n",
    "print(\"Transformed Test data\")\n",
    "print(pd.concat([ts1,tr.iloc[:,-1:]],axis=1))\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Encoder\n",
    "Similar to one hot encoder but converts number to direct binary digits. \n",
    "\n",
    "Advantages: Better than OHE in that this will produce only 7 columns for 100 levels whereas OHE produces 100 columns for the same 100 levels. Helpful for high cardinal categories\n",
    "\n",
    "Disadvantages : Not useful for lower cardinal categories since number of columns produced is almost equal to number of levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "   Country_0  Country_1  Country_2  prob  Target\n",
      "0          0          0          1   0.9       1\n",
      "1          0          1          0   0.6       0\n",
      "2          0          0          1   0.7       1\n",
      "3          0          1          1   0.2       0\n",
      "4          1          0          0   0.4       0\n",
      "Transformed Test data\n",
      "   Country_0  Country_1  Country_2  prob  Target\n",
      "0          0          0          1   0.5       1\n",
      "1          0          1          0   0.2       0\n",
      "2          0          0          1   0.4       1\n",
      "3          0          0          0   0.3       0\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [0 0 0 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 2]\n",
      " [0 0]]\n",
      "Accuracy Score\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "binary = ce.BinaryEncoder(cols=['Country'])\n",
    "tr1=binary.fit_transform(tr)\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr1.iloc[:,-1:]\n",
    "\n",
    "print(\"Transformed Training data\")\n",
    "print(tr1)\n",
    "ts1=binary.transform(ts)\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts1.iloc[:,-1:]\n",
    "\n",
    "print(\"Transformed Test data\")\n",
    "print(ts1)\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base N Encoder\n",
    "Base-N encoder encodes the categories into arrays of their base-N representation.\n",
    "A base of 1 is equivalent to one-hot encoding, a base of 2 is equivalent to binary encoding.\n",
    "N=number of actual categories is equivalent to vanilla ordinal encoding.\n",
    "\n",
    "Advantages :\n",
    "Number of columns can be controlled using N\n",
    "\n",
    "Disadvantages:\n",
    "Another parameter to control. Might take multiple runs to decide the best N for each category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "   Country_0  Country_1  prob  Target\n",
      "0          0          1   0.9       1\n",
      "1          0          2   0.6       0\n",
      "2          0          1   0.7       1\n",
      "3          0          3   0.2       0\n",
      "4          1          0   0.4       0\n",
      "Transformed Test data\n",
      "   Country_0  Country_1  prob  Target\n",
      "0          0          1   0.5       1\n",
      "1          0          2   0.2       0\n",
      "2          0          1   0.4       1\n",
      "3          0          0   0.3       0\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [0 0 0 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 2]\n",
      " [0 0]]\n",
      "Accuracy Score\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "ce_basen = ce.BaseNEncoder(cols=['Country'],base=4)\n",
    "tr1=ce_basen.fit_transform(tr)\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr1.iloc[:,-1:]\n",
    "print(\"Transformed Training data\")\n",
    "print(tr1)\n",
    "\n",
    "ts1=ce_basen.transform(ts)\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts1.iloc[:,-1:]\n",
    "print(\"Transformed Test data\")\n",
    "print(ts1)\n",
    "\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helmert Encoder \n",
    "\n",
    "Compares each level of a categorical variable to the mean of the subsequent levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "   intercept  Country_0  Country_1  Country_2  prob\n",
      "0          1       -1.0       -1.0       -1.0   0.9\n",
      "1          1        1.0       -1.0       -1.0   0.6\n",
      "2          1       -1.0       -1.0       -1.0   0.7\n",
      "3          1        0.0        2.0       -1.0   0.2\n",
      "4          1        0.0        0.0        3.0   0.4\n",
      "Transformed Test data\n",
      "   intercept  Country_0  Country_1  Country_2  prob\n",
      "0          1       -1.0       -1.0       -1.0   0.5\n",
      "1          1        1.0       -1.0       -1.0   0.2\n",
      "2          1       -1.0       -1.0       -1.0   0.4\n",
      "3          1        0.0        0.0        0.0   0.3\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [1 0 1 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 0]\n",
      " [0 2]]\n",
      "Accuracy Score\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "hashen= ce.HelmertEncoder(cols=['Country'])\n",
    "hashen.fit(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "tr1=hashen.transform(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr.iloc[:,-1:]\n",
    "print(\"Transformed Training data\")\n",
    "print(tr1)\n",
    "\n",
    "\n",
    "ts1=hashen.transform(ts.iloc[:,:-1],ts.iloc[:,-1:])\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts.iloc[:,-1:]\n",
    "print(\"Transformed Test data\")\n",
    "print(ts1)\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target encoder \n",
    "\n",
    "Encodes mean value of the target for each level i.e., features are replaced with a blend of posterior probability of the target given particular categorical value and the prior probability of the target over all the training data.\n",
    "\n",
    "Similar to encoding probability of the targets based on each level except that sklearn takes prior probability into account. This prior probability is used for smoothing which provides reasonable weights to less occuring levels and also does not skew the weights towards high occuring levels.\n",
    "\n",
    "Advantages : Quick and does not add extra dimensionality\n",
    "\n",
    "Disadvantages : Prone to overfitting. Does not encode unseen values. Notice the 'NaN'. Dependent on the target distribution of the training dataset\n",
    "\n",
    "2 more hyperparameters to tune : \n",
    "\n",
    "min_samples_leaf : minimum samples to take category average into account\n",
    "\n",
    "smoothing : smoothing effect to balance categorical average vs prior. Higher value means stronger regularization. The value must be strictly bigger than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "    Country  prob  Target\n",
      "0  0.838635   0.9       1\n",
      "1  0.400000   0.6       0\n",
      "2  0.838635   0.7       1\n",
      "3  0.400000   0.2       0\n",
      "4  0.400000   0.4       0\n",
      "Transformed Test data\n",
      "    Country  prob  Target\n",
      "0  0.838635   0.5       1\n",
      "1  0.400000   0.2       0\n",
      "2  0.838635   0.4       1\n",
      "3  0.400000   0.3       0\n",
      "4       NaN   NaN       0\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [1 0 1 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 0]\n",
      " [0 2]]\n",
      "Accuracy Score\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TgtEn= ce.TargetEncoder(cols=['Country'])\n",
    "TgtEn.fit(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "tr1=TgtEn.transform(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr.iloc[:,-1:]\n",
    "print(\"Transformed Training data\")\n",
    "print(pd.concat([tr1,tr.iloc[:,-1:]],axis=1))\n",
    "\n",
    "\n",
    "ts1=TgtEn.transform(ts.iloc[:,:-1],ts.iloc[:,-1:])\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts.iloc[:,-1:]\n",
    "print(\"Transformed Test data\")\n",
    "print(pd.concat([ts1,tr.iloc[:,-1:]],axis=1))\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Out Encoding\n",
    "\n",
    "This is very similar to target encoding but excludes the current row’s target when calculating the mean target value for a level.\n",
    "\n",
    "Advantages : Reduces the effect of outliers.\n",
    "\n",
    "Disadvantages : Prone to overfitting. Induces higher variance in the encoded values. Does not encode unseen values. Notice 'NaN'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "   Country  prob  Target\n",
      "0      1.0   0.9       1\n",
      "1      0.4   0.6       0\n",
      "2      1.0   0.7       1\n",
      "3      0.4   0.2       0\n",
      "4      0.4   0.4       0\n",
      "Transformed Test data\n",
      "   Country  prob  Target\n",
      "0      1.0   0.5       1\n",
      "1      0.4   0.2       0\n",
      "2      1.0   0.4       1\n",
      "3      0.4   0.3       0\n",
      "4      NaN   NaN       0\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [1 0 1 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 0]\n",
      " [0 2]]\n",
      "Accuracy Score\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#Target dependent\n",
    "leaveout= ce.LeaveOneOutEncoder(cols=['Country'])\n",
    "leaveout.fit(tr.iloc[:,:-1],tr.iloc[:,-1:].values.ravel())\n",
    "tr1=leaveout.transform(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr.iloc[:,-1:]\n",
    "print(\"Transformed Training data\")\n",
    "print(pd.concat([tr1,tr.iloc[:,-1:]],axis=1))\n",
    "\n",
    "\n",
    "ts1=leaveout.transform(ts.iloc[:,:-1],ts.iloc[:,-1:])\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts.iloc[:,-1:]\n",
    "print(\"Transformed Test data\")\n",
    "print(pd.concat([ts1,tr.iloc[:,-1:]],axis=1))\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost Encoder\n",
    "\n",
    "Similar to leave one out Encoder but calculates values 'on-the-fly'. Introduces a concept of \"time\" in a way that it calculates values on the fly. Precaution should be taken to introduce randomisation if dataset is ordered according to target. Runs many iterations to avoid overfitting.\n",
    "\n",
    "Advantages : Prevents overfitting\n",
    "\n",
    "Disadvantages : Might take longer since it runs on many iterations. Does not encode unseen values. Notice the 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "   Country  prob  Target\n",
      "0      0.4   0.9       1\n",
      "1      0.4   0.6       0\n",
      "2      0.7   0.7       1\n",
      "3      0.4   0.2       0\n",
      "4      0.4   0.4       0\n",
      "Transformed Test data\n",
      "   Country  prob  Target\n",
      "0      0.4   0.5       1\n",
      "1      0.4   0.2       0\n",
      "2      0.7   0.4       1\n",
      "3      0.4   0.3       0\n",
      "4      NaN   NaN       0\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [0 0 1 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 1]\n",
      " [0 1]]\n",
      "Accuracy Score\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "#Target dependent\n",
    "catboost= ce.CatBoostEncoder(cols=['Country'])\n",
    "catboost.fit(tr.iloc[:,:-1],tr.iloc[:,-1:].values.ravel())\n",
    "tr1=catboost.transform(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr.iloc[:,-1:]\n",
    "print(\"Transformed Training data\")\n",
    "print(pd.concat([tr1,tr.iloc[:,-1:]],axis=1))\n",
    "\n",
    "\n",
    "ts1=catboost.transform(ts.iloc[:,:-1],ts.iloc[:,-1:])\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts.iloc[:,-1:]\n",
    "print(\"Transformed Test data\")\n",
    "print(pd.concat([ts1,tr.iloc[:,-1:]],axis=1))\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Difference Encoder\n",
    "The mean of the target for a level is compared with the mean of the target for the prior level. \n",
    "\n",
    "Produces n columns given n is the number of levels in the category\n",
    "\n",
    "Advantages : encodes target information which will improve model's accuracy\n",
    "\n",
    "Disadvantages : Produces many columns \n",
    "\n",
    "Refer [here](https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/#backward) for detailed workedout example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "   intercept  Country_0  Country_1  Country_2  prob  Target\n",
      "0          1      -0.75       -0.5      -0.25   0.9       1\n",
      "1          1       0.25       -0.5      -0.25   0.6       0\n",
      "2          1      -0.75       -0.5      -0.25   0.7       1\n",
      "3          1       0.25        0.5      -0.25   0.2       0\n",
      "4          1       0.25        0.5       0.75   0.4       0\n",
      "Transformed Test data\n",
      "   intercept  Country_0  Country_1  Country_2  prob  Target\n",
      "0          1      -0.75       -0.5      -0.25   0.5       1\n",
      "1          1       0.25       -0.5      -0.25   0.2       0\n",
      "2          1      -0.75       -0.5      -0.25   0.4       1\n",
      "3          1       0.00        0.0       0.00   0.3       0\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [0 0 0 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 2]\n",
      " [0 0]]\n",
      "Accuracy Score\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "BD_en=ce.BackwardDifferenceEncoder(cols=['Country'])\n",
    "tr1=BD_en.fit_transform(tr)\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr1.iloc[:,-1:]\n",
    "print(\"Transformed Training data\")\n",
    "print(tr1)\n",
    "ts1=BD_en.transform(ts)\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts1.iloc[:,-1:]\n",
    "print(\"Transformed Test data\")\n",
    "print(ts1)\n",
    "\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### James-Stein Encoder\n",
    "\n",
    "\n",
    "Returns the weighted average of mean target value for that level and the mean target value.\n",
    "Might not be effective as it works only for Normal distributions.\n",
    "Requires target normalisation for binary targets which is similar to WOE \n",
    "\n",
    "More info [here](http://contrib.scikit-learn.org/category_encoders/jamesstein.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "   Country  prob\n",
      "0      1.0   0.9\n",
      "1      0.0   0.6\n",
      "2      1.0   0.7\n",
      "3      0.0   0.2\n",
      "4      0.0   0.4\n",
      "Transformed Test data\n",
      "   Country  prob\n",
      "0      1.0   0.5\n",
      "1      0.0   0.2\n",
      "2      1.0   0.4\n",
      "3      0.4   0.3\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [1 0 1 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 0]\n",
      " [0 2]]\n",
      "Accuracy Score\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "JSen= ce.JamesSteinEncoder(cols=['Country'])\n",
    "JSen.fit(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "tr1=JSen.transform(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr.iloc[:,-1:]\n",
    "print(\"Transformed Training data\")\n",
    "print(tr1)\n",
    "\n",
    "ts1=JSen.transform(ts.iloc[:,:-1],ts.iloc[:,-1:])\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts.iloc[:,-1:]\n",
    "print(\"Transformed Test data\")\n",
    "print(ts1)\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-Estimate\n",
    "\n",
    "It is a simplified version of target encoder in that it only has 1 hyper parameter to tune (m). Higher values of m results in stronger shrinkage (reduces the effect of the stronger level) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "   Country  prob\n",
      "0      0.8   0.9\n",
      "1      0.2   0.6\n",
      "2      0.8   0.7\n",
      "3      0.2   0.2\n",
      "4      0.2   0.4\n",
      "Transformed Test data\n",
      "   Country  prob\n",
      "0      0.8   0.5\n",
      "1      0.2   0.2\n",
      "2      0.8   0.4\n",
      "3      0.4   0.3\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [1 0 1 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 0]\n",
      " [0 2]]\n",
      "Accuracy Score\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MEst= ce.MEstimateEncoder(cols=['Country'])\n",
    "MEst.fit(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "tr1=MEst.transform(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr.iloc[:,-1:]\n",
    "print(\"Transformed Training data\")\n",
    "print(tr1)\n",
    "\n",
    "ts1=MEst.transform(ts.iloc[:,:-1],ts.iloc[:,-1:])\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts.iloc[:,-1:]\n",
    "print(\"Transformed Test data\")\n",
    "print(ts1)\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Encoder\n",
    "\n",
    "Trend analysis in that it is looking for the linear, quadratic and cubic trends in the categorical variable.\n",
    "Suitable for ordinal variables in which the levels are equally spaced, like intervals or buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "   intercept  Country_0  Country_1  Country_2  prob\n",
      "0          1  -0.670820        0.5  -0.223607   0.9\n",
      "1          1  -0.223607       -0.5   0.670820   0.6\n",
      "2          1  -0.670820        0.5  -0.223607   0.7\n",
      "3          1   0.223607       -0.5  -0.670820   0.2\n",
      "4          1   0.670820        0.5   0.223607   0.4\n",
      "Transformed Test data\n",
      "   intercept  Country_0  Country_1  Country_2  prob\n",
      "0          1  -0.670820        0.5  -0.223607   0.5\n",
      "1          1  -0.223607       -0.5   0.670820   0.2\n",
      "2          1  -0.670820        0.5  -0.223607   0.4\n",
      "3          1   0.000000        0.0   0.000000   0.3\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [1 0 1 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 0]\n",
      " [0 2]]\n",
      "Accuracy Score\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "PolEs= ce.PolynomialEncoder(cols=['Country'])\n",
    "PolEs.fit(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "tr1=PolEs.transform(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr.iloc[:,-1:]\n",
    "print(\"Transformed Training data\")\n",
    "print(tr1)\n",
    "\n",
    "ts1=PolEs.transform(ts.iloc[:,:-1],ts.iloc[:,-1:])\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts.iloc[:,-1:]\n",
    "print(\"Transformed Test data\")\n",
    "print(ts1)\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight of Evidence \n",
    "Target based encoder. It is the measure of the “strength” of a grouping technique to separate good and bad.\n",
    "Supports only binomial targets \n",
    "\n",
    "Weight of evidence is calculated as follows. \n",
    "\n",
    "WoE = ln(a / b)\n",
    "\n",
    "a = Distribution of Good Credit Outcomes\n",
    "\n",
    "b = Distribution of Bad Credit Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Training data\n",
      "    Country  prob\n",
      "0  1.321756   0.9\n",
      "1  0.000000   0.6\n",
      "2  1.321756   0.7\n",
      "3  0.000000   0.2\n",
      "4  0.000000   0.4\n",
      "Transformed Test data\n",
      "    Country  prob\n",
      "0  1.321756   0.5\n",
      "1  0.000000   0.2\n",
      "2  1.321756   0.4\n",
      "3  0.000000   0.3\n",
      "Target\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Predicted: [1 0 1 0]\n",
      "Confusion Matrix \n",
      "  0 1\n",
      "[[2 0]\n",
      " [0 2]]\n",
      "Accuracy Score\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "WOE= ce.WOEEncoder(cols=['Country'])\n",
    "WOE.fit(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "tr1=WOE.transform(tr.iloc[:,:-1],tr.iloc[:,-1:])\n",
    "xtr=tr1.iloc[:,:-1]\n",
    "ytr=tr.iloc[:,-1:]\n",
    "print(\"Transformed Training data\")\n",
    "print(tr1)\n",
    "\n",
    "\n",
    "ts1=WOE.transform(ts.iloc[:,:-1],ts.iloc[:,-1:])\n",
    "xts=ts1.iloc[:,:-1]\n",
    "yts=ts.iloc[:,-1:]\n",
    "print(\"Transformed Test data\")\n",
    "print(ts1)\n",
    "train_classifier(xtr,ytr,xts,yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLMM Encoders and Hashing encoder had trouble loading. Hashing encoder required re-installation of older version of Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. Scikit learn - http://contrib.scikit-learn.org/category_encoders/count.html\n",
    "2. Worked out example for few of the encoding techniques : https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/\n",
    "3. Source code : https://github.com/scikit-learn-contrib/category_encoders/tree/master/category_encoders\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
